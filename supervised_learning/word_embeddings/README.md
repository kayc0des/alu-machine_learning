# Word Embeddings

In Natural Language Processing (NLP), computers cannot inherently understand human language. To bridge this gap, text data must be converted into numerical representations that can be processed by machine learning (ML) or deep learning (DL) models. These representations are known as word embeddings.

`Word embeddings` capture the meaning and context of words in a vector form, making them interpretable by models. There are several types of word embeddings, including:

- One-Hot Encoding
- Bag of Words (BoW)
- Term Frequency-Inverse Document Frequency (TF-IDF)
- Word2Vec and other advanced models

This repository will explore some word embedding techniques used in converting words to vectors.